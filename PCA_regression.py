#NOTE FOR GALVANIZE:  This program still has the same output as the previous program
#but instead of doing the bulk of the work in this program I have left it for the 
#completePCA class

#This program puts the completePCA class to the test by using the compete data.  The goal of this program is
#to run a regression with average stay(how many seconds someone stays on a website)
#on one PCA factor.  I chose the first PCA factor because it accounted for 0.9997344548956167% of the "variation" in the data
#I acquired this number from the summary dictionary outputted by the PCA function in PCA_func_JWB_version
#The program creates an instance of the competePCA class and then calls the linreg method
#for each type of way the feature vectors can be generated (condPCA, myOwnPCA and sklearn).
#The goal of this whole project has not necessarily been to generate an insightful regression, but
#rather validate my two PCA functions and utilize some common libraries and techniques used in data
#science.  I believe I accomplished this since the three regressions using separate PCA factors from each
#function come out very close to one another. Therefore validating both of the PCA functions I created.

#NOTE:  The output for the myOwnPCA version of the linear regression is slightly different than
#the other two regressions.  The eigenvalues are essentially identical between the myOwnPCA and
#the condPCA eigenvalues, though slight differences exist between the eigenvectors, which is 
#obviously the culprit for the slight discrepensy in regression outputs.  I have checked the math and 
#my code but since approximately identical eigenvalues are being generated I do not believe myOwnPCA
#to be fundamentally incorrect.  I believe the discrepensy in eigenvectors might be due to either
#rounding problems caused by all the multiplication in the QR algorithm or possibly the stopping
#condition needs to be altered so that a more accurate estimate of the eigenvectors may be generated.
#Either way I believe the fact that the regression output from myOwnPCA and the other two types of
#generation, along with the eigenvalues being essentially identical still shows myOwnPCA is valid
#but that more accurate results are generated by either condPCA or sklearn
#===================================================================================
from completePCA import completePCA
import pandas as pd 
import sqlite3 as lite
#===================================================================================
#connect database and place the SQL data into a dataframe
con=lite.connect('/Users/jacobbaumbach/Desktop/PCA/competedata.db')
cur=con.cursor()
df=pd.read_sql("SELECT * from competedata", con, index_col='url')
#===================================================================================
#initialize completePCA class with our dataframe of data, the name of the column containing
#our dependent data and the tolerance that will be used in the myOwnPCA class
#a list is also initialized to hold our regression results
instance=completePCA(df,'avgstay',0.1)
regResults=[]
#===================================================================================
#loop through each way we can generate the feature vectors and perform a linear regression
#with each of the the different generations of the feature vectors and store the regression output
#for each iteration in regResults (we are using one feature vector for reasons explained in
# the preamble)
for i in range(0,3):
	regResults.append(instance.linReg(i,1))

