#NOTE FOR GALVANIZE: This class essentially takes over most of the work that was originally
#done in PCA_regression specifically the three ways the features vectors can be generated,
#performing the linear regressions and now the dependent variable and the number of feature
#variables is exogenous and not predetermined as it was in PCA_Regression

#This class initializes the dependent variable, independent variables and tolerance used in 
#the myOwnPCA class the linReg is the primary method which one passes the way they want the 
#feature vectors generated(0=condPCA, 1=myOwnPCA, else=sklearn) and the number of feature vectors
#they would like used for independent variables.  The linReg method calls the feat method which 
#generates the specified number of feature vectors via the specified way one wants the features
#generated.  If one wants the features generated by 0 or 1 then eigGen is called to generate the
#eigenvectors by the appropriate class and then matvec is called to convert the independent
#variable data and eigenvectors into feature vectors
#===================================================================================
from PCA_func_JWB_version import myOwnPCA
from condensed_PCA import condPCA
import numpy as np 
import statsmodels.api as sm
from sklearn import decomposition

class completePCA(myOwnPCA, condPCA):
	#initializes the dependent variable, self.y, for the regression by entering in a string, y, of one of the
	#column names of the dataframe, d.  The dependent variable is dropped from the dataframe, d, and
	#the independent variables are initialized, self.x, and the tolerance, self.tol, that maybe used for generating
	#the eigenvalues and vectors if the myOwnPCA class if chosen is initialized with the value chosen for tol
	def __init__ (self,d,y,tol):
		self.y=d[y]
		d.drop(y,axis=1,inplace=True)
		self.x=d
		self.tol=tol
	#multiplies independent variables by the chosen number of eigenvectors indicated
	#by num (in that order) producing a matrix of feature vectors that will be used in
	#the regression
	def matvec(self,eigvec,num):
		col=list(self.x)
		c={i:[0.0 for j in range(0,len(self.x[col[0]]))] for i in range(0,num)}
		for i in range(0,num):
			for j in range(0,len(self.x[col[0]])):
				for k in range(0,len(col)):
					c[i][j]+=self.x[col[k]][j]*eigvec[i][k]
		return c
	#generates the eigenvectors either by the condPCA method of myOwnPCA method indicated
	#by PCAchoice
	def eigGen(self, PCAchoice):
		if PCAchoice==0:
			methd=condPCA(self.x)
			eig,eigvec=methd.PCAcond()
		else:
			methd=myOwnPCA(self.x)
			eig,eigvec,summary=methd.PCA(self.tol)
		return eigvec
	#generates number of feature vectors indicated by users choice of num via condPCA, myOwnPCA
	#or the sklearn method
	def feat(self,PCAchoice,num):
		if PCAchoice==0 or PCAchoice==1:
			eigvec=self.eigGen(PCAchoice)
			ft=self.matvec(eigvec,num)
			ftmat=np.matrix([[ft[i][j] for j in range(0,len(ft[0]))] for i in range(0,len(ft))]).transpose()
		else:
			mat=self.x.as_matrix()
			pca=decomposition.PCA(n_components=num)
			ft=pca.fit_transform(mat)
			ftmat=np.matrix(ft)
		return ftmat
	#performs a linear regression on the chosen dependent variable and the number of feature vectors
	#indicated by num, and the feature vectors are generated either by condPCA, myOwnPCA or the sklearn
	#method based on user input for PCAchoice
	def linReg(self,PCAchoice,num):
		ftmat=self.feat(PCAchoice,num)
		Y=np.matrix(self.y).transpose()
		X=sm.add_constant(ftmat)
		model=sm.OLS(Y,X)
		f=model.fit()
		print f.summary()
		return f